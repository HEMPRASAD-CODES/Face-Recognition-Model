{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1624149,"sourceType":"datasetVersion","datasetId":959963},{"sourceId":12152514,"sourceType":"datasetVersion","datasetId":7653475},{"sourceId":12152551,"sourceType":"datasetVersion","datasetId":7653495},{"sourceId":12152580,"sourceType":"datasetVersion","datasetId":7653506},{"sourceId":12152628,"sourceType":"datasetVersion","datasetId":7653537},{"sourceId":12152652,"sourceType":"datasetVersion","datasetId":7653554},{"sourceId":12154906,"sourceType":"datasetVersion","datasetId":7655098},{"sourceId":12155108,"sourceType":"datasetVersion","datasetId":7655225},{"sourceId":12155247,"sourceType":"datasetVersion","datasetId":7655312},{"sourceId":12155281,"sourceType":"datasetVersion","datasetId":7655335},{"sourceId":12155511,"sourceType":"datasetVersion","datasetId":7655486},{"sourceId":12155557,"sourceType":"datasetVersion","datasetId":7655518},{"sourceId":12155762,"sourceType":"datasetVersion","datasetId":7655665},{"sourceId":434233,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":354096,"modelId":375404}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Add this cell at the beginning of your Kaggle notebook\n!pip install face-recognition\n!pip install mtcnn\n!pip install keras-facenet\nimport cv2\nimport numpy as np\nimport os\nimport pickle\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nimport argparse\nfrom PIL import Image, ImageEnhance\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport cv2\n    \ndef show_image(image, title=\"Image\"):\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.figure(figsize=(10, 8))\n        plt.imshow(rgb_image)\n        plt.title(title)\n        plt.axis('off')\n        plt.show()\n\n# Try to import face_recognition library (most reliable)\ntry:\n    import face_recognition\n    FACE_RECOGNITION_AVAILABLE = True\n    print(\"✓ Using face_recognition library (recommended)\")\nexcept ImportError:\n    FACE_RECOGNITION_AVAILABLE = False\n    print(\"⚠ face_recognition not available. Install with: pip install face-recognition\")\n\n# Try to import MTCNN for face detection\ntry:\n    from mtcnn import MTCNN\n    MTCNN_AVAILABLE = True\n    print(\"✓ MTCNN face detector available\")\nexcept ImportError:\n    MTCNN_AVAILABLE = False\n    print(\"⚠ MTCNN not available. Install with: pip install mtcnn tensorflow\")\n\n# Try to import FaceNet model\ntry:\n    from keras_facenet import FaceNet\n    FACENET_AVAILABLE = True\n    print(\"✓ FaceNet model available\")\nexcept ImportError:\n    FACENET_AVAILABLE = False\n    print(\"⚠ FaceNet not available. Install with: pip install keras-facenet\")\n\n# Deep learning imports\ntry:\n    import torch\n    import torch.nn as nn\n    import torchvision.transforms as transforms\n    from sklearn.metrics.pairwise import cosine_similarity\n    TORCH_AVAILABLE = True\nexcept ImportError:\n    TORCH_AVAILABLE = False\n    print(\"⚠ PyTorch not available for enhanced features\")\n\n\nclass ProfessionalFaceRecognitionSystem:\n    \"\"\"Professional Face Recognition System using state-of-the-art models\"\"\"\n    \n    def __init__(self, known_faces_dir=\"/kaggle/working/face-recognition-dataset/Original Images/Original Images\", encodings_file=\"/kaggle/working/face-encoding/face_encodings.pkl\"):\n        if known_faces_dir is None:\n            # Check what's actually in your Kaggle input\n            input_dirs = os.listdir(\"/kaggle/input\")\n            print(f\"Available input directories: {input_dirs}\")\n            \n            # Try to find the face recognition dataset\n            for dirname in input_dirs:\n                full_path = f\"/kaggle/input/{dirname}\"\n                if os.path.isdir(full_path):\n                    # Check if it contains subdirectories (person folders)\n                    subdirs = [d for d in os.listdir(full_path) if os.path.isdir(os.path.join(full_path, d))]\n                    if len(subdirs) > 0:\n                        known_faces_dir = full_path\n                        print(f\"Found dataset at: {known_faces_dir}\")\n                        print(f\"Contains folders: {subdirs}\")\n                        break\n            \n            if known_faces_dir is None:\n                known_faces_dir = \"/kaggle/input\"\n                print(\"Using default input directory\")\n        self.known_faces_dir = Path(known_faces_dir)\n        self.encodings_file = encodings_file\n        self.known_face_encodings = []\n        self.known_face_names = []\n        \n        # Create directories\n        self.known_faces_dir.mkdir(exist_ok=True)\n        \n        # Initialize face detection and recognition models\n        self.init_models()\n        \n        # Load or create encodings\n        self.load_or_create_encodings()\n    \n    def init_models(self):\n        \"\"\"Initialize the best available face detection and recognition models\"\"\"\n        print(\"Initializing face detection and recognition models...\")\n        \n        # Method 1: face_recognition library (most reliable)\n        if FACE_RECOGNITION_AVAILABLE:\n            self.detection_method = \"face_recognition\"\n            self.encoding_method = \"face_recognition\"\n            print(\"✓ Using face_recognition library for detection and encoding\")\n            return\n        \n        # Method 2: MTCNN + FaceNet\n        if MTCNN_AVAILABLE and FACENET_AVAILABLE:\n            self.detector = MTCNN()\n            self.embedder = FaceNet()\n            self.detection_method = \"mtcnn\"\n            self.encoding_method = \"facenet\"\n            print(\"✓ Using MTCNN + FaceNet combination\")\n            return\n        \n        # Method 3: MTCNN + Custom embeddings\n        if MTCNN_AVAILABLE:\n            self.detector = MTCNN()\n            self.detection_method = \"mtcnn\"\n            self.encoding_method = \"custom\"\n            print(\"✓ Using MTCNN detector with custom embeddings\")\n            return\n        \n        # Fallback: OpenCV DNN face detector\n        self.init_opencv_dnn()\n        self.detection_method = \"opencv_dnn\"\n        self.encoding_method = \"custom\"\n    \n    def init_opencv_dnn(self):\n        \"\"\"Initialize OpenCV DNN face detector\"\"\"\n        try:\n            # Download models if needed\n            self.download_opencv_models()\n            \n            # Load the DNN model\n            self.net = cv2.dnn.readNetFromTensorflow(\n                'models/opencv_face_detector_uint8.pb',\n                'models/opencv_face_detector.pbtxt'\n            )\n            print(\"✓ OpenCV DNN face detector loaded\")\n        except Exception as e:\n            print(f\"Failed to load OpenCV DNN: {e}\")\n            # Ultimate fallback to Haar Cascade\n            self.face_cascade = cv2.CascadeClassifier(\n                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n            )\n            self.detection_method = \"haar\"\n            print(\"✓ Using Haar Cascade as final fallback\")\n    \n    def download_opencv_models(self):\n        \"\"\"Download OpenCV face detection models\"\"\"\n        import urllib.request\n        \n        models_dir = Path(\"models\")\n        models_dir.mkdir(exist_ok=True)\n        \n        models = {\n            'opencv_face_detector_uint8.pb': 'https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/opencv_face_detector_uint8.pb',\n            'opencv_face_detector.pbtxt': 'https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/opencv_face_detector.pbtxt'\n        }\n        \n        for filename, url in models.items():\n            filepath = models_dir / filename\n            if not filepath.exists():\n                print(f\"Downloading {filename}...\")\n                try:\n                    urllib.request.urlretrieve(url, filepath)\n                    print(f\"✓ Downloaded {filename}\")\n                except Exception as e:\n                    print(f\"Failed to download {filename}: {e}\")\n    \n    def detect_faces(self, image):\n        \"\"\"Detect faces using the best available method\"\"\"\n        if self.detection_method == \"face_recognition\":\n            return self.detect_faces_face_recognition(image)\n        elif self.detection_method == \"mtcnn\":\n            return self.detect_faces_mtcnn(image)\n        elif self.detection_method == \"opencv_dnn\":\n            return self.detect_faces_opencv_dnn(image)\n        else:\n            return self.detect_faces_haar(image)\n    \n    def detect_faces_face_recognition(self, image):\n        \"\"\"Detect faces using face_recognition library\"\"\"\n        # Convert BGR to RGB\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        face_locations = face_recognition.face_locations(rgb_image, model=\"hog\")\n        \n        # Convert to our format (x, y, w, h)\n        faces = []\n        for (top, right, bottom, left) in face_locations:\n            faces.append((left, top, right - left, bottom - top, 1.0))\n        \n        return faces\n    \n    def detect_faces_mtcnn(self, image):\n        \"\"\"Detect faces using MTCNN\"\"\"\n        # Convert BGR to RGB\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        result = self.detector.detect_faces(rgb_image)\n        \n        faces = []\n        for face in result:\n            if face['confidence'] > 0.9:  # High confidence threshold\n                x, y, w, h = face['box']\n                faces.append((x, y, w, h, face['confidence']))\n        \n        return faces\n    \n    def detect_faces_opencv_dnn(self, image):\n        \"\"\"Detect faces using OpenCV DNN\"\"\"\n        h, w = image.shape[:2]\n        blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123])\n        self.net.setInput(blob)\n        detections = self.net.forward()\n        \n        faces = []\n        for i in range(detections.shape[2]):\n            confidence = detections[0, 0, i, 2]\n            if confidence > 0.7:  # Higher confidence threshold\n                x1 = int(detections[0, 0, i, 3] * w)\n                y1 = int(detections[0, 0, i, 4] * h)\n                x2 = int(detections[0, 0, i, 5] * w)\n                y2 = int(detections[0, 0, i, 6] * h)\n                faces.append((x1, y1, x2 - x1, y2 - y1, confidence))\n        \n        return faces\n    \n    def detect_faces_haar(self, image):\n        \"\"\"Detect faces using Haar Cascade\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(\n            gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50)\n        )\n        return [(x, y, w, h, 1.0) for (x, y, w, h) in faces]\n    \n    def extract_face_encoding(self, image, face_location=None):\n        \"\"\"Extract face encoding using the best available method\"\"\"\n        if self.encoding_method == \"face_recognition\":\n            return self.extract_encoding_face_recognition(image, face_location)\n        elif self.encoding_method == \"facenet\":\n            return self.extract_encoding_facenet(image, face_location)\n        else:\n            return self.extract_encoding_custom(image, face_location)\n    \n    def extract_encoding_face_recognition(self, image, face_location):\n        \"\"\"Extract face encoding using face_recognition library\"\"\"\n        # Convert BGR to RGB\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if face_location:\n            x, y, w, h = face_location[:4]\n            # Convert to face_recognition format (top, right, bottom, left)\n            face_locations = [(y, x + w, y + h, x)]\n        else:\n            face_locations = face_recognition.face_locations(rgb_image)\n        \n        if not face_locations:\n            return None\n        \n        # Get the encoding for the first face\n        encodings = face_recognition.face_encodings(rgb_image, face_locations)\n        if encodings:\n            return encodings[0]\n        \n        return None\n    \n    def extract_encoding_facenet(self, image, face_location):\n        \"\"\"Extract face encoding using FaceNet\"\"\"\n        if face_location:\n            x, y, w, h = face_location[:4]\n            face_image = image[y:y+h, x:x+w]\n        else:\n            face_image = image\n        \n        # Preprocess for FaceNet\n        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n        face_image = cv2.resize(face_image, (160, 160))\n        face_image = np.expand_dims(face_image, axis=0)\n        face_image = face_image.astype('float32') / 255.0\n        \n        try:\n            embedding = self.embedder.embeddings(face_image)\n            return embedding[0]\n        except Exception as e:\n            print(f\"FaceNet encoding error: {e}\")\n            return None\n    \n    def extract_encoding_custom(self, image, face_location):\n        \"\"\"Extract custom face encoding using traditional methods\"\"\"\n        if face_location:\n            x, y, w, h = face_location[:4]\n            face_image = image[y:y+h, x:x+w]\n        else:\n            face_image = image\n        \n        # Enhanced preprocessing\n        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n        \n        # Normalize lighting\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        enhanced = clahe.apply(gray)\n        \n        # Resize to standard size\n        resized = cv2.resize(enhanced, (128, 128))\n        \n        # Extract multiple features\n        # 1. Raw pixel values\n        pixel_features = resized.flatten()\n        \n        # 2. LBP features\n        lbp_features = self.extract_lbp_features(resized)\n        \n        # 3. Gradient features\n        grad_x = cv2.Sobel(resized, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(resized, cv2.CV_64F, 0, 1, ksize=3)\n        gradient_features = np.concatenate([grad_x.flatten(), grad_y.flatten()])\n        \n        # Combine all features\n        combined_features = np.concatenate([\n            pixel_features * 0.5,  # Reduced weight for raw pixels\n            lbp_features * 2.0,    # Higher weight for LBP\n            gradient_features * 0.3  # Lower weight for gradients\n        ])\n        \n        # Normalize\n        norm = np.linalg.norm(combined_features)\n        if norm > 0:\n            combined_features = combined_features / norm\n        \n        return combined_features.astype(np.float32)\n    \n    def extract_lbp_features(self, image):\n        \"\"\"Extract Local Binary Pattern features\"\"\"\n        # Simple LBP implementation\n        rows, cols = image.shape\n        lbp = np.zeros((rows, cols), dtype=np.uint8)\n        \n        for i in range(1, rows - 1):\n            for j in range(1, cols - 1):\n                center = image[i, j]\n                code = 0\n                \n                # 8-neighborhood\n                neighbors = [\n                    image[i-1, j-1], image[i-1, j], image[i-1, j+1],\n                    image[i, j+1], image[i+1, j+1], image[i+1, j],\n                    image[i+1, j-1], image[i, j-1]\n                ]\n                \n                for k, neighbor in enumerate(neighbors):\n                    if neighbor >= center:\n                        code |= (1 << k)\n                \n                lbp[i, j] = code\n        \n        # Create histogram\n        hist, _ = np.histogram(lbp.ravel(), bins=256, range=(0, 256))\n        return hist.astype(np.float32)\n    \n    def load_or_create_encodings(self):\n        \"\"\"Load existing encodings or create new ones\"\"\"\n        if os.path.exists(self.encodings_file):\n            print(\"Loading existing face encodings...\")\n            try:\n                with open(self.encodings_file, 'rb') as f:\n                    data = pickle.load(f)\n                    self.known_face_encodings = data.get('encodings', [])\n                    self.known_face_names = data.get('names', [])\n                \n                unique_people = len(set(self.known_face_names))\n                print(f\"✓ Loaded encodings for {unique_people} people ({len(self.known_face_encodings)} total)\")\n                \n                if unique_people == 0:\n                    print(\"No valid encodings found, creating new ones...\")\n                    self.create_face_encodings()\n                    \n            except Exception as e:\n                print(f\"Error loading encodings: {e}\")\n                self.create_face_encodings()\n        else:\n            print(\"Creating new face encodings...\")\n            self.create_face_encodings()\n    \n    def create_face_encodings(self):\n        \"\"\"Create face encodings from known faces directory\"\"\"\n        print(f\"Scanning for known faces in {self.known_faces_dir}...\")\n        \n        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n        encodings = []\n        names = []\n        \n        if not any(self.known_faces_dir.iterdir()):\n            print(f\"⚠ No directories found in {self.known_faces_dir}\")\n            print(\"Please create subdirectories with person names and place their images inside.\")\n            return\n        \n        for person_dir in self.known_faces_dir.iterdir():\n            if person_dir.is_dir():\n                person_name = person_dir.name\n                print(f\"\\nProcessing {person_name}...\")\n                \n                person_images = list(person_dir.glob('*'))\n                person_images = [img for img in person_images if img.suffix.lower() in image_extensions]\n                \n                if not person_images:\n                    print(f\"  ⚠ No valid images found for {person_name}\")\n                    continue\n                \n                person_encodings = []\n                successful_images = 0\n                \n                for img_path in person_images:\n                    try:\n                        # Load image\n                        image = cv2.imread(str(img_path))\n                        if image is None:\n                            print(f\"  ✗ Could not load {img_path.name}\")\n                            continue\n                        \n                        # Detect faces\n                        faces = self.detect_faces(image)\n                        \n                        if not faces:\n                            print(f\"  ✗ No face detected in {img_path.name}\")\n                            continue\n                        \n                        if len(faces) > 1:\n                            print(f\"  ⚠ Multiple faces in {img_path.name}, using largest\")\n                            faces = [max(faces, key=lambda x: x[2] * x[3])]  # Largest face\n                        \n                        # Extract encoding\n                        face_location = faces[0]\n                        encoding = self.extract_face_encoding(image, face_location)\n                        \n                        if encoding is not None:\n                            person_encodings.append(encoding)\n                            successful_images += 1\n                            print(f\"  ✓ Processed {img_path.name}\")\n                        else:\n                            print(f\"  ✗ Could not extract encoding from {img_path.name}\")\n                            \n                    except Exception as e:\n                        print(f\"  ✗ Error processing {img_path.name}: {e}\")\n                \n                # Add encodings for this person\n                if person_encodings:\n                    encodings.extend(person_encodings)\n                    names.extend([person_name] * len(person_encodings))\n                    print(f\"  → Successfully processed {successful_images}/{len(person_images)} images\")\n                else:\n                    print(f\"  ✗ No valid encodings created for {person_name}\")\n        \n        self.known_face_encodings = encodings\n        self.known_face_names = names\n        \n        if encodings:\n            self.save_encodings()\n            unique_people = len(set(names))\n            print(f\"\\n✓ Database created: {unique_people} people, {len(encodings)} total encodings\")\n        else:\n            print(\"\\n⚠ No face encodings were created!\")\n            print(\"Please check that:\")\n            print(\"1. Images are in person-name subdirectories\")\n            print(\"2. Images contain clear, front-facing faces\")\n            print(\"3. Image files are in supported formats (jpg, png, etc.)\")\n    \n    def save_encodings(self):\n        \"\"\"Save encodings to file\"\"\"\n        data = {\n            'encodings': self.known_face_encodings,\n            'names': self.known_face_names,\n            'metadata': {\n                'created': datetime.now().isoformat(),\n                'detection_method': self.detection_method,\n                'encoding_method': self.encoding_method,\n                'total_people': len(set(self.known_face_names)),\n                'total_encodings': len(self.known_face_encodings)\n            }\n        }\n        \n        with open(self.encodings_file, 'wb') as f:\n            pickle.dump(data, f)\n        \n        print(f\"✓ Encodings saved to {self.encodings_file}\")\n    \n    def recognize_face(self, face_encoding, threshold=None):\n        \"\"\"Recognize a face from its encoding\"\"\"\n        if not self.known_face_encodings or face_encoding is None:\n            return \"Unknown\", 0.0\n        \n        # Set threshold based on encoding method\n        if threshold is None:\n            if self.encoding_method == \"face_recognition\":\n                threshold = 0.6  # face_recognition uses distance, lower is better\n            else:\n                threshold = 0.7  # Custom encodings use similarity, higher is better\n        \n        if self.encoding_method == \"face_recognition\":\n            # Use face_recognition's built-in comparison\n            distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n            \n            # Group by person and find best match\n            person_distances = {}\n            for i, distance in enumerate(distances):\n                name = self.known_face_names[i]\n                if name not in person_distances:\n                    person_distances[name] = []\n                person_distances[name].append(distance)\n            \n            # Find person with minimum average distance\n            best_person = None\n            best_distance = float('inf')\n            \n            for name, dist_list in person_distances.items():\n                avg_distance = np.mean(dist_list)\n                if avg_distance < best_distance:\n                    best_distance = avg_distance\n                    best_person = name\n            \n            # Convert distance to similarity for consistent output\n            similarity = max(0, 1.0 - best_distance)\n            \n            if best_distance <= threshold:\n                return best_person, similarity\n            else:\n                return \"Unknown\", similarity\n                \n        else:\n            # Use cosine similarity for custom encodings\n            similarities = cosine_similarity([face_encoding], self.known_face_encodings)[0]\n            \n            # Group by person\n            person_similarities = {}\n            for i, sim in enumerate(similarities):\n                name = self.known_face_names[i]\n                if name not in person_similarities:\n                    person_similarities[name] = []\n                person_similarities[name].append(sim)\n            \n            # Find person with highest average similarity\n            best_person = None\n            best_similarity = -1\n            \n            for name, sim_list in person_similarities.items():\n                avg_similarity = np.mean(sim_list)\n                if avg_similarity > best_similarity:\n                    best_similarity = avg_similarity\n                    best_person = name\n            \n            if best_similarity >= threshold:\n                return best_person, best_similarity\n            else:\n                return \"Unknown\", best_similarity\n    \n\n    \n    def recognize_faces_in_image(self, image_path, threshold=None):\n        \"\"\"Recognize all faces in an image and display with bounding boxes and name tags (matplotlib, no save).\"\"\"\n        try:\n            image = cv2.imread(str(image_path))\n            if image is None:\n                print(f\"Could not load image: {image_path}\")\n                return []\n    \n            # Detect faces\n            faces = self.detect_faces(image)\n            if not faces:\n                print(\"No faces detected in the image\")\n                return []\n    \n            results = []\n            print(f\"Found {len(faces)} face(s), processing...\")\n    \n            for i, face_data in enumerate(faces):\n                x, y, w, h = face_data[:4]\n                confidence = face_data[4] if len(face_data) > 4 else 1.0\n    \n                # Skip very small faces\n                if w < 40 or h < 40:\n                    print(f\"Skipping small face {i+1}: {w}x{h}\")\n                    continue\n    \n                # Extract face encoding\n                encoding = self.extract_face_encoding(image, (x, y, w, h))\n    \n                if encoding is not None:\n                    # Recognize face\n                    name, similarity = self.recognize_face(encoding, threshold)\n    \n                    results.append({\n                        'face_id': i + 1,\n                        'name': name,\n                        'confidence': similarity,\n                        'detection_confidence': confidence,\n                        'location': (x, y, x + w, y + h),\n                        'size': (w, h)\n                    })\n    \n                    print(f\"Face {i+1}: {name} (confidence: {similarity:.3f})\")\n    \n                    # Draw bounding box\n                    color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n                    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n    \n                    # Prepare label\n                    label = f\"{name} ({similarity:.2f})\" if name != \"Unknown\" else f\"Unknown ({similarity:.2f})\"\n                    label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n                    label_y = y - 10 if y - 10 > 10 else y + 10\n                    cv2.rectangle(image, (x, label_y - label_size[1] - 5), (x + label_size[0], label_y + 5), color, cv2.FILLED)\n                    cv2.putText(image, label, (x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n                else:\n                    print(f\"Could not extract encoding for face {i+1}\")\n    \n            # Show the image with bounding boxes and labels using matplotlib\n            show_image(image, title=\"Recognized Faces\")\n    \n            return results\n    \n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n            return []\n\n\n    def recognize_faces_in_video_notebook(self, video_source=0, threshold=None, max_frames=100):\n            cap = cv2.VideoCapture(video_source)\n            if not cap.isOpened():\n                print(f\"Error: Could not open video source {video_source}\")\n                return\n        \n            frame_count = 0\n            while frame_count < max_frames:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n        \n                faces = self.detect_faces(frame)\n                for i, face_data in enumerate(faces):\n                    x, y, w, h = face_data[:4]\n                    confidence = face_data[4] if len(face_data) > 4 else 1.0\n                    if w < 40 or h < 40:\n                        continue\n                    encoding = self.extract_face_encoding(frame, (x, y, w, h))\n                    if encoding is not None:\n                        name, similarity = self.recognize_face(encoding, threshold)\n                        color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n                        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n                        label = f\"{name} ({similarity:.2f})\"\n                        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n                        label_y = y - 10 if y - 10 > 10 else y + 10\n                        cv2.rectangle(frame, (x, label_y - label_size[1] - 5), (x + label_size[0], label_y + 5), color, cv2.FILLED)\n                        cv2.putText(frame, label, (x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n        \n                # Display frame using matplotlib\n                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                plt.figure(figsize=(10, 8))\n                plt.imshow(rgb_frame)\n                plt.axis('off')\n                plt.title(f\"Frame {frame_count+1}\")\n                plt.show()\n        \n                frame_count += 1\n                # Optional: break early for demo\n                # if frame_count > 10: break\n        \n            cap.release()\n\n        \n    \n    def add_person(self, name, image_path):\n        \"\"\"Add a new person from an image\"\"\"\n        try:\n            image = cv2.imread(str(image_path))\n            if image is None:\n                print(f\"Could not load image: {image_path}\")\n                return False\n            \n            faces = self.detect_faces(image)\n            \n            if not faces:\n                print(\"No face detected in the image\")\n                return False\n            \n            if len(faces) > 1:\n                print(\"Multiple faces detected, using the largest one\")\n                faces = [max(faces, key=lambda x: x[2] * x[3])]\n            \n            # Extract encoding\n            face_location = faces[0]\n            encoding = self.extract_face_encoding(image, face_location)\n            \n            if encoding is None:\n                print(\"Could not extract face encoding\")\n                return False\n            \n            # Add to database\n            self.known_face_encodings.append(encoding)\n            self.known_face_names.append(name)\n            \n            # Save updated database\n            self.save_encodings()\n            \n            print(f\"✓ Added {name} to the database\")\n            return True\n            \n        except Exception as e:\n            print(f\"Error adding person: {e}\")\n            return False\n    \n    def remove_person(self, name):\n        \"\"\"Remove all encodings for a person\"\"\"\n        indices_to_remove = [i for i, n in enumerate(self.known_face_names) if n == name]\n        \n        if not indices_to_remove:\n            print(f\"Person '{name}' not found in database\")\n            return False\n        \n        # Remove in reverse order to maintain indices\n        for i in reversed(indices_to_remove):\n            del self.known_face_encodings[i]\n            del self.known_face_names[i]\n        \n        self.save_encodings()\n        print(f\"✓ Removed {len(indices_to_remove)} encodings for '{name}'\")\n        return True\n    \n    def list_people(self):\n        \"\"\"List all people in the database\"\"\"\n        if not self.known_face_names:\n            print(\"No people in the database\")\n            return\n        \n        # Count encodings per person\n        person_counts = {}\n        for name in self.known_face_names:\n            person_counts[name] = person_counts.get(name, 0) + 1\n        \n        print(\"\\nPeople in Database:\")\n        print(\"-\" * 40)\n        for name, count in sorted(person_counts.items()):\n            print(f\"  {name}: {count} encoding(s)\")\n        \n        print(f\"\\nTotal: {len(person_counts)} people, {len(self.known_face_names)} encodings\")\n    \n    def get_system_info(self):\n        \"\"\"Get system information\"\"\"\n        return {\n            'detection_method': self.detection_method,\n            'encoding_method': self.encoding_method,\n            'known_people': len(set(self.known_face_names)),\n            'total_encodings': len(self.known_face_encodings),\n            'available_libraries': {\n                'face_recognition': FACE_RECOGNITION_AVAILABLE,\n                'mtcnn': MTCNN_AVAILABLE,\n                'facenet': FACENET_AVAILABLE,\n                'torch': TORCH_AVAILABLE\n            }\n        }\n    \n    def benchmark_recognition(self, test_image_path, iterations=10):\n        \"\"\"Benchmark recognition performance\"\"\"\n        import time\n        \n        try:\n            image = cv2.imread(str(test_image_path))\n            if image is None:\n                print(f\"Could not load test image: {test_image_path}\")\n                return\n            \n            print(f\"Benchmarking with {iterations} iterations...\")\n            \n            # Face detection benchmark\n            start_time = time.time()\n            for _ in range(iterations):\n                faces = self.detect_faces(image)\n            detection_time = (time.time() - start_time) / iterations\n            \n            if not faces:\n                print(\"No faces detected for benchmarking\")\n                return\n            \n            # Face encoding benchmark\n            face_location = faces[0]\n            start_time = time.time()\n            for _ in range(iterations):\n                encoding = self.extract_face_encoding(image, face_location)\n            encoding_time = (time.time() - start_time) / iterations\n            \n            # Recognition benchmark\n            if encoding is not None and self.known_face_encodings:\n                start_time = time.time()\n                for _ in range(iterations):\n                    name, similarity = self.recognize_face(encoding)\n                recognition_time = (time.time() - start_time) / iterations\n            else:\n                recognition_time = 0\n            \n            print(f\"\\nBenchmark Results:\")\n            print(f\"  Face Detection: {detection_time*1000:.2f}ms per frame\")\n            print(f\"  Face Encoding: {encoding_time*1000:.2f}ms per face\")\n            print(f\"  Face Recognition: {recognition_time*1000:.2f}ms per face\")\n            print(f\"  Total Pipeline: {(detection_time + encoding_time + recognition_time)*1000:.2f}ms per face\")\n            \n        except Exception as e:\n            print(f\"Benchmark error: {e}\")\n\n\n# Main execution and CLI interface\ndef main():\n    parser = argparse.ArgumentParser(description='Professional Face Recognition System')\n    parser.add_argument('--mode', choices=['train', 'recognize', 'video', 'add', 'remove', 'list', 'info', 'benchmark'],\n                       default='recognize', help='Operation mode')\n    parser.add_argument('--image', type=str, help='Path to image file')\n    parser.add_argument('--video', type=str, help='Path to video file or camera index (0, 1, etc.)')\n    parser.add_argument('--output', type=str, help='Output video file path')\n    parser.add_argument('--name', type=str, help='Person name for add/remove operations')\n    parser.add_argument('--threshold', type=float, help='Recognition threshold')\n    parser.add_argument('--known-faces', type=str, default='known_faces', \n                       help='Directory containing known faces')\n    parser.add_argument('--encodings', type=str, default='face_encodings.pkl',\n                       help='Face encodings file')\n    \n    args = parser.parse_args()\n    \n    # Initialize the system\n    print(\"Initializing Professional Face Recognition System...\")\n    system = ProfessionalFaceRecognitionSystem(args.known_faces, args.encodings)\n    \n    if args.mode == 'train':\n        print(\"Creating/updating face encodings database...\")\n        system.create_face_encodings()\n        \n    elif args.mode == 'recognize':\n        if not args.image:\n            print(\"Please specify an image file with --image\")\n            return\n        \n        print(f\"Recognizing faces in: {args.image}\")\n        results = system.recognize_faces_in_image(args.image, args.threshold)\n        \n        if results:\n            print(f\"\\nRecognition Results:\")\n            print(\"-\" * 50)\n            for result in results:\n                print(f\"Face {result['face_id']}: {result['name']} \"\n                      f\"(confidence: {result['confidence']:.3f}, \"\n                      f\"location: {result['location']}, \"\n                      f\"size: {result['size']})\")\n        else:\n            print(\"No faces recognized\")\n    \n    elif args.mode == 'video':\n        video_source = 0  # Default to webcam\n        if args.video:\n            if args.video.isdigit():\n                video_source = int(args.video)\n            else:\n                video_source = args.video\n        \n        print(f\"Starting video recognition from: {video_source}\")\n        system.recognize_faces_video(video_source, args.output, args.threshold)\n    \n    elif args.mode == 'add':\n        if not args.name or not args.image:\n            print(\"Please specify both --name and --image for adding a person\")\n            return\n        \n        success = system.add_person(args.name, args.image)\n        if success:\n            print(f\"Successfully added {args.name}\")\n        else:\n            print(f\"Failed to add {args.name}\")\n    \n    elif args.mode == 'remove':\n        if not args.name:\n            print(\"Please specify --name for removing a person\")\n            return\n        \n        success = system.remove_person(args.name)\n        if success:\n            print(f\"Successfully removed {args.name}\")\n        else:\n            print(f\"Failed to remove {args.name}\")\n    \n    elif args.mode == 'list':\n        system.list_people()\n    \n    elif args.mode == 'info':\n        info = system.get_system_info()\n        print(\"\\nSystem Information:\")\n        print(\"-\" * 40)\n        print(f\"Detection Method: {info['detection_method']}\")\n        print(f\"Encoding Method: {info['encoding_method']}\")\n        print(f\"Known People: {info['known_people']}\")\n        print(f\"Total Encodings: {info['total_encodings']}\")\n        print(\"\\nAvailable Libraries:\")\n        for lib, available in info['available_libraries'].items():\n            status = \"✓\" if available else \"✗\"\n            print(f\"  {status} {lib}\")\n    \n    elif args.mode == 'benchmark':\n        if not args.image:\n            print(\"Please specify an image file with --image for benchmarking\")\n            return\n        \n        system.benchmark_recognition(args.image)\n\n\n# Usage examples and documentation\ndef print_usage_examples():\n    \"\"\"Print usage examples\"\"\"\n    examples = \"\"\"\nUsage Examples:\n    \n# Train the system (create face database)\npython face_recognition_system.py --mode train\n\n# Recognize faces in an image\npython face_recognition_system.py --mode recognize --image path/to/image.jpg\n\n# Real-time recognition from webcam\npython face_recognition_system.py --mode video\n\n# Recognize from video file\npython face_recognition_system.py --mode video --video path/to/video.mp4\n\n# Add a new person\npython face_recognition_system.py --mode add --name \"John Doe\" --image path/to/john.jpg\n\n# Remove a person\npython face_recognition_system.py --mode remove --name \"John Doe\"\n\n# List all known people\npython face_recognition_system.py --mode list\n\n# Show system information\npython face_recognition_system.py --mode info\n\n# Benchmark performance\npython face_recognition_system.py --mode benchmark --image path/to/test.jpg\n\nSetup Instructions:\n1. Create a 'known_faces' directory\n2. Create subdirectories for each person (e.g., 'known_faces/John_Doe/')\n3. Place multiple photos of each person in their respective directories\n4. Run training mode to create the face database\n5. Use recognition modes to identify faces\n\nRequired Libraries (install as needed):\n- pip install opencv-python numpy pillow\n- pip install face-recognition (recommended)\n- pip install mtcnn tensorflow (alternative)\n- pip install keras-facenet (alternative)\n- pip install torch torchvision (for enhanced features)\n- pip install scikit-learn (for similarity calculations)\n    \"\"\"\n    print(examples)\n\n\n# For Kaggle notebook usage\nif __name__ == \"__main__\":\n    # Initialize the system\n    print(\"Initializing Face Recognition System for Kaggle...\")\n    system = ProfessionalFaceRecognitionSystem()\n    \n    # List what we found\n    print(f\"Dataset directory: {system.known_faces_dir}\")\n    if system.known_faces_dir.exists():\n        dirs = [d.name for d in system.known_faces_dir.iterdir() if d.is_dir()]\n        print(f\"Found person directories: {dirs}\")\n    \n    # You can now run specific operations:\n    # system.create_face_encodings()  # To train\n    # system.list_people()  # To see who's in the database\n    # results = system.recognize_faces_in_image(\"path/to/test/image.jpg\")  # To recognize\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:37:53.813955Z","iopub.execute_input":"2025-06-13T15:37:53.814222Z","iopub.status.idle":"2025-06-13T15:38:48.593431Z","shell.execute_reply.started":"2025-06-13T15:37:53.814198Z","shell.execute_reply":"2025-06-13T15:38:48.592521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.list_people()  # To see who's in the database\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/testimages/v1.jpg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:38:55.694616Z","iopub.execute_input":"2025-06-13T15:38:55.695758Z","iopub.status.idle":"2025-06-13T15:38:57.622963Z","shell.execute_reply.started":"2025-06-13T15:38:55.695717Z","shell.execute_reply":"2025-06-13T15:38:57.622162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/alexandradaddario/ad1.jpg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:01.074377Z","iopub.execute_input":"2025-06-13T15:39:01.074666Z","iopub.status.idle":"2025-06-13T15:39:01.534644Z","shell.execute_reply.started":"2025-06-13T15:39:01.074643Z","shell.execute_reply":"2025-06-13T15:39:01.533960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/henrycavill/hc1.webp\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:05.386575Z","iopub.execute_input":"2025-06-13T15:39:05.386848Z","iopub.status.idle":"2025-06-13T15:39:05.696675Z","shell.execute_reply.started":"2025-06-13T15:39:05.386827Z","shell.execute_reply":"2025-06-13T15:39:05.695901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/henrycavill/hc2.jpg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:12.215816Z","iopub.execute_input":"2025-06-13T15:39:12.216110Z","iopub.status.idle":"2025-06-13T15:39:12.350530Z","shell.execute_reply.started":"2025-06-13T15:39:12.216087Z","shell.execute_reply":"2025-06-13T15:39:12.349709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/rock-zac/rock-zef.jpg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:15.283917Z","iopub.execute_input":"2025-06-13T15:39:15.284517Z","iopub.status.idle":"2025-06-13T15:39:15.326556Z","shell.execute_reply.started":"2025-06-13T15:39:15.284494Z","shell.execute_reply":"2025-06-13T15:39:15.325984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/zac-rock/a.webp\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:24.896492Z","iopub.execute_input":"2025-06-13T15:39:24.896758Z","iopub.status.idle":"2025-06-13T15:39:26.769744Z","shell.execute_reply.started":"2025-06-13T15:39:24.896736Z","shell.execute_reply":"2025-06-13T15:39:26.768919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/viratk/121615492.avif\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:20.643658Z","iopub.execute_input":"2025-06-13T15:39:20.644131Z","iopub.status.idle":"2025-06-13T15:39:20.657764Z","shell.execute_reply.started":"2025-06-13T15:39:20.644106Z","shell.execute_reply":"2025-06-13T15:39:20.657162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.recognize_faces_in_video_notebook(\"/kaggle/input/vk12345/2025-06-13 18-01-00.mkv\",max_frames=10)  # 0 for webcam, or video file path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:39:34.394798Z","iopub.execute_input":"2025-06-13T15:39:34.395308Z","iopub.status.idle":"2025-06-13T15:39:49.658617Z","shell.execute_reply.started":"2025-06-13T15:39:34.395274Z","shell.execute_reply":"2025-06-13T15:39:49.657824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Source and destination\nsrc = \"/kaggle/input/face-recognition-dataset\"\ndst = \"/kaggle/working/face-recognition-dataset\"\n\n# Copy entire folder\nshutil.copytree(src, dst)\nprint(\"Dataset copied successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport shutil\n\n# Source and destination\nsrc = \"/kaggle/input/vanjikodi\"\ndst = \"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi\"\n\n# Copy entire folder\nshutil.copytree(src, dst)\nprint(\"Dataset copied successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Source and destination\nsrc = \"/kaggle/input/face-encodings\"\ndst = \"/kaggle/working/face-encoding\"\n\n# Copy entire folder\nshutil.copytree(src, dst)\nprint(\"Dataset copied successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\", \"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.38 PM (1).jpeg\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.38 PM (2).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.38 PM.jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.39 PM (1).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.39 PM (2).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.39 PM.jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.40 PM (1).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.40 PM (2).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.40 PM.jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.41 PM (1).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.41 PM (2).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.41 PM (3).jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.41 PM.jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system.add_person(\"VanjiKodi\",\"/kaggle/working/face-recognition-dataset/Original Images/Original Images/Vanjikodi/WhatsApp Image 2025-06-13 at 6.12.42 PM.jpeg\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/vanjitest/WhatsApp Image 2025-06-13 at 6.12.37 PM.jpeg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:40:06.556732Z","iopub.execute_input":"2025-06-13T15:40:06.556999Z","iopub.status.idle":"2025-06-13T15:40:07.293031Z","shell.execute_reply.started":"2025-06-13T15:40:06.556978Z","shell.execute_reply":"2025-06-13T15:40:07.292258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = system.recognize_faces_in_image(\"/kaggle/input/attai1/WhatsApp Image 2025-06-13 at 6.50.11 PM.jpeg\")  # To recognize\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T15:40:11.979130Z","iopub.execute_input":"2025-06-13T15:40:11.979813Z","iopub.status.idle":"2025-06-13T15:40:12.771020Z","shell.execute_reply.started":"2025-06-13T15:40:11.979775Z","shell.execute_reply":"2025-06-13T15:40:12.770344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the entire working directory\nshutil.make_archive('/kaggle/working', 'zip', '/kaggle/working')\n\n# Then use this to download\nfrom IPython.display import FileLink\nFileLink(r'/kaggle/working/my_data_backup.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Folder to be zipped\nfolder_to_zip = '/kaggle/working/face-recognition-dataset/Original Images/Original Images'\n\n# Destination zip path\nzip_path = '/kaggle/working/my_data_backup1.zip'\n\n# Create the zip file\nshutil.make_archive(base_name=zip_path.replace('.zip', ''), format='zip', root_dir=folder_to_zip)\n\nprint(\"Zip file created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:19:40.325083Z","iopub.execute_input":"2025-06-13T14:19:40.325818Z","iopub.status.idle":"2025-06-13T14:20:09.246153Z","shell.execute_reply.started":"2025-06-13T14:19:40.325790Z","shell.execute_reply":"2025-06-13T14:20:09.245314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/my_data_backup.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/working'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:20:19.600404Z","iopub.execute_input":"2025-06-13T14:20:19.601120Z","iopub.status.idle":"2025-06-13T14:20:19.605074Z","shell.execute_reply.started":"2025-06-13T14:20:19.601095Z","shell.execute_reply":"2025-06-13T14:20:19.604453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('/kaggle/working/my_data_backup1.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:20:33.381284Z","iopub.execute_input":"2025-06-13T14:20:33.382014Z","iopub.status.idle":"2025-06-13T14:20:33.386859Z","shell.execute_reply.started":"2025-06-13T14:20:33.381987Z","shell.execute_reply":"2025-06-13T14:20:33.386108Z"}},"outputs":[],"execution_count":null}]}